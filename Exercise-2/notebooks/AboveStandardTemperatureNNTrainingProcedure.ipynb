{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "huYorh9BtZWD"
      },
      "outputs": [],
      "source": [
        "# Imports \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import sys, random\n",
        "from sklearn.metrics import (mean_absolute_error,\n",
        "                             mean_squared_error,\n",
        "                             median_absolute_error,\n",
        "                             max_error, r2_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Declarations \n",
        "\n",
        "class DataMaker(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Prepare Dataset for regression model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        self.targets = X.astype(np.float32)\n",
        "        self.labels = y.astype(np.float32)\n",
        "        return\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.targets[i, :], self.labels[i]\n",
        "\n",
        "class TimeOverMaxTemperatureNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Network Model for predicting the maximum time\n",
        "    over 723 C of a welding process\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input=8, output=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, output)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def predict(self,x):\n",
        "      with torch.no_grad():\n",
        "        x = torch.tensor(x)\n",
        "        prediction = self.forward(x).detach().cpu().numpy()\n",
        "      return prediction"
      ],
      "metadata": {
        "id": "DbsyElHNtdPS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER PARAMETERS\n",
        "torch.manual_seed(42)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEBUG = False"
      ],
      "metadata": {
        "id": "eOrWmiTmthnx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load custom dataset from drive \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "DATA_PATH = \"/content/drive/My Drive/ΔΠΜΣ/Α Εξάμηνο/Προηγμένα Συστήματα Κατεργασιών/Ασκήσεις/Ασκηση 2/data/nn2/\"\n",
        "sys.path.append(DATA_PATH)"
      ],
      "metadata": {
        "id": "HabtJHMEtiLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets into pandas DataFrame and split inputs/targets\n",
        "train_df = pd.read_csv(DATA_PATH + \"train_dataset.csv\")\n",
        "X_train, Y_train = train_df.iloc[:, 0:-1], train_df.iloc[:, -1]\n",
        "\n",
        "validation_df = pd.read_csv(DATA_PATH + \"validation_dataset.csv\")\n",
        "X_validation, Y_validation = validation_df.iloc[:, 0:-1], validation_df.iloc[:, -1]\n",
        "\n",
        "test_df = pd.read_csv(DATA_PATH + \"test_dataset.csv\")\n",
        "X_test, Y_test = test_df.iloc[:, 0:-1], test_df.iloc[:, -1]"
      ],
      "metadata": {
        "id": "lL2dGDVPtkve"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply pre-processing in respect to training values (mean and std)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train).astype(\"float32\")\n",
        "\n",
        "# NOTE: use mean and std of training set \n",
        "X_validation_scaled = scaler.transform(X_validation).astype(\"float32\")\n",
        "X_test_scaled = scaler.transform(X_test).astype(\"float32\")\n",
        "\n",
        "Y_train_scaled = Y_train.astype(\"float32\")\n",
        "Y_validation_scaled = Y_validation.astype(\"float32\")\n",
        "Y_test_scaled = Y_test.astype(\"float32\")\n",
        "\n",
        "# Check if it works \n",
        "if (DEBUG):\n",
        "  print(\"Means of training set:\\n\", X_train_scaled.mean(axis=0), \"\\n\")\n",
        "  print(\"Standard deviations of training set:\\n\", X_train_scaled.std(axis=0), \"\\n\\n\")\n",
        "\n",
        "  print(\"Means of validation set:\\n\", X_validation_scaled.mean(axis=0), \"\\n\")\n",
        "  print(\"Standard deviations of validation set:\\n\", X_validation_scaled.std(axis=0), \"\\n\\n\")\n",
        "\n",
        "  print(\"Means of test set:\\n\", X_test_scaled.mean(axis=0), \"\\n\")\n",
        "  print(\"Standard deviations of test set:\\n\", X_test_scaled.std(axis=0), \"\\n\\n\")"
      ],
      "metadata": {
        "id": "6_oAMfhBtmcU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for NN \n",
        "train_dataset = DataMaker(X_train_scaled, Y_train_scaled)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "\n",
        "validation_dataset = DataMaker(X_validation_scaled, Y_validation_scaled)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "Y_SzWBnDtnzl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Neural Network model\n",
        "input_size = len(X_train.columns)\n",
        "model = TimeOverMaxTemperatureNN(input = input_size, output = 1)\n",
        "model = model.to(device)\n",
        "criterion = nn.functional.mse_loss\n",
        "# criterion = nn.functional.l1_loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "if (DEBUG):\n",
        "  print(model)"
      ],
      "metadata": {
        "id": "tBLXsvU5to8d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_losses , total_validation_losses= [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  training_progress_bar = tqdm.notebook.tqdm(train_loader, leave=False)\n",
        "  validation_progress_bar = tqdm.notebook.tqdm(validation_loader, leave=False)\n",
        "  train_loss, valid_loss = 0.0, 0.0\n",
        "  \n",
        "  # *==== Training Procedure ====* \n",
        "  model.train()\n",
        "  for data, targets in training_progress_bar:\n",
        "    data, targets = data.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(data)\n",
        "    loss = criterion(y_pred, torch.unsqueeze(targets, dim=1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    training_progress_bar.set_description(f'Training Loss: {loss.item():.3f}')\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # *==== Validation Procedure ====*\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for data, targets in validation_progress_bar:\n",
        "      data, targets = data.to(device), targets.to(device)\n",
        "      y_pred = model(data)\n",
        "      targets = torch.unsqueeze(targets, dim=1)\n",
        "      loss = criterion(y_pred,targets)\n",
        "      validation_progress_bar.set_description(f'Validation Loss: {loss.item():.3f}')\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    # If the validation loss of the model is lower than that of all the\n",
        "    # previous epochs, save the model state\n",
        "    mean_val_loss = valid_loss / len(validation_loader)\n",
        "    if (epoch == 0):\n",
        "      torch.save(model.state_dict(), \"./NN_dt_best_model_parameters.pt\")\n",
        "    elif (epoch > 0) and (mean_val_loss < np.min(total_validation_losses)):\n",
        "      print(\"Model Selection!\")\n",
        "      torch.save(model.state_dict(), \"./NN_dt_best_model_parameters.pt\")\n",
        "\n",
        "  if (epoch % 10 == 0) :\n",
        "    message = f'Epoch {epoch} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(validation_loader)}'\n",
        "    tqdm.tqdm.write(message)\n",
        "\n",
        "  total_train_losses.append(train_loss / len(train_loader))\n",
        "  total_validation_losses.append(valid_loss / len(validation_loader))"
      ],
      "metadata": {
        "id": "yBHJvlpstqTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print validation/training/testing losses \n",
        "epochs_list = [e+1 for e in range(EPOCHS)]\n",
        "plt.plot(epochs_list, total_train_losses, label=\"Training\")\n",
        "plt.plot(epochs_list, total_validation_losses, label=\"Validation\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efFZ9zMRtr5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Evaluation\n",
        "model.load_state_dict(torch.load(\"./NN_dt_best_model_parameters.pt\"))\n",
        "model.to(\"cpu\")\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Performance report\")\n",
        "\n",
        "print(f\"Mean Squared Error: {mean_squared_error(Y_test_scaled, y_pred):.2f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(Y_test_scaled, y_pred):.2f}\")\n",
        "print(f\"Median Absolute Error: {median_absolute_error(Y_test_scaled, y_pred):.2f}\")\n",
        "print(f\"Max Error: {max_error(Y_test_scaled, y_pred):.2f}\")\n",
        "print(f\"R2 score: {r2_score(Y_test_scaled, y_pred):.2f}\")"
      ],
      "metadata": {
        "id": "UEqnVRXGttIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(Y_test_scaled, y_pred, fmt='bo', label=\"True values\")\n",
        "plt.xlabel(\"True Max Temperature\")\n",
        "plt.ylabel(\"Predicted Max Temperature\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "DEhx5dWEtuZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}